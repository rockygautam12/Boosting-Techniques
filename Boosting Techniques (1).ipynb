{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e258c63-24c1-4d4d-9ea9-d0be93b51d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1: What is Boosting in Machine Learning? Explain how it improves weak\n",
    "learners\n",
    "\n",
    "What is Boosting in Machine Learning?\n",
    "Boosting is an ensemble learning technique that combines multiple weak learners (models that perform slightly better than random guessing) to form a strong learner with high accuracy.\n",
    "\n",
    "It works sequentially — each new model is trained to correct the errors made by the previous models.\n",
    "\n",
    "Key Idea:\n",
    "Focus more on difficult-to-predict data points.\n",
    "\n",
    "Reduce bias and variance by combining many improved weak learners.\n",
    "\n",
    "Final prediction is made by weighted voting (classification) or weighted averaging (regression).\n",
    "\n",
    "How Boosting Improves Weak Learners\n",
    "Start with a weak learner\n",
    "\n",
    "Usually a shallow decision tree (1–5 depth).\n",
    "\n",
    "Train it on the data and evaluate errors.\n",
    "\n",
    "Assign higher weights to misclassified examples so the next learner pays more attention to them.\n",
    "\n",
    "Train the next weak learner using this updated weighting.\n",
    "\n",
    "Repeat the process for many rounds, each time improving on the mistakes of the previous model.\n",
    "\n",
    "Combine all weak learners into a strong model by weighted voting/averaging.\n",
    "\n",
    "Why Boosting Works\n",
    "Reduces bias: Weak learners on their own may underfit; sequential corrections make them fit complex patterns.\n",
    "\n",
    "Focuses learning: By giving more weight to hard cases, it adapts to the problem structure.\n",
    "\n",
    "Model diversity: Each learner is slightly different due to the weighted data distribution.\n",
    "\n",
    "Popular Boosting Algorithms\n",
    "AdaBoost (Adaptive Boosting) – adjusts sample weights after each learner.\n",
    "\n",
    "Gradient Boosting – fits new learners to the residual errors of the previous model.\n",
    "\n",
    "XGBoost / LightGBM / CatBoost – optimized, faster gradient boosting variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec4338-01d5-45bb-a4c0-31ba2cd3eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms\n",
    "of how models are trained?\n",
    "\n",
    "| Feature               | **AdaBoost (Adaptive Boosting)**                                                         | **Gradient Boosting**                                                                                  |\n",
    "| --------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |\n",
    "| **Main Idea**         | Adjusts **sample weights** after each round so the next learner focuses on harder cases. | Fits the next learner to the **residual errors** (negative gradients) of the previous model.           |\n",
    "| **Error Handling**    | Misclassified samples get **higher weights**, correctly classified get lower weights.    | Errors are treated as residuals — the next model directly predicts these residuals.                    |\n",
    "| **Weight Updates**    | Updates **data point weights** + **model weights** based on accuracy.                    | Keeps data point weights fixed, updates the model by minimizing a loss function (e.g., MSE, log loss). |\n",
    "| **Loss Function**     | Implicitly minimizes **exponential loss**.                                               | Can use different loss functions (MSE, MAE, log loss, custom).                                         |\n",
    "| **Learning Process**  | Weighted data → Train weak learner → Compute error → Update weights → Repeat.            | Original data → Predict residuals → Fit new learner to residuals → Add to ensemble → Repeat.           |\n",
    "| **Final Prediction**  | Weighted vote (classification) or weighted sum (regression).                             | Sum of all learners’ predictions.                                                                      |\n",
    "| **Example Algorithm** | Classic AdaBoost (Freund & Schapire, 1996).                                              | Gradient Boosted Decision Trees (GBDT), XGBoost, LightGBM.                                             |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    In Simple Terms\n",
    "AdaBoost: \"I’ll reweight the data so you pay more attention to the mistakes.\"\n",
    "\n",
    "Gradient Boosting: \"I’ll make you predict the mistakes directly and keep improving on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cfd993-d70c-429f-b6dd-da396f53351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3: How does regularization help in XGBoost?\n",
    "\n",
    "How Regularization Helps in XGBoost\n",
    "XGBoost (Extreme Gradient Boosting) includes built-in regularization to control model complexity and prevent overfitting, which is often a problem in boosting methods.\n",
    "\n",
    "Types of Regularization in XGBoost\n",
    "L1 Regularization (alpha)\n",
    "\n",
    "Adds a penalty proportional to the absolute value of leaf weights.\n",
    "\n",
    "Encourages sparsity (more zero weights) → simpler models.\n",
    "\n",
    "Effect: Removes weak features by driving their contribution to 0.\n",
    "\n",
    "L2 Regularization (lambda)\n",
    "\n",
    "Adds a penalty proportional to the square of leaf weights.\n",
    "\n",
    "Shrinks large weights → makes the model less sensitive to noise.\n",
    "\n",
    "Effect: Stabilizes model predictions.\n",
    "\n",
    "Tree Complexity Control\n",
    "\n",
    "gamma: Minimum loss reduction needed to make a split.\n",
    "\n",
    "Higher gamma → fewer splits → simpler trees.\n",
    "\n",
    "max_depth, min_child_weight, subsample, colsample_bytree\n",
    "\n",
    "Limit tree size, control sampling → reduce overfitting.\n",
    "\n",
    "How It Helps\n",
    "Prevents Overfitting → Penalizes overly complex trees that memorize training data.\n",
    "\n",
    "Improves Generalization → Keeps the model simple enough to perform well on unseen data.\n",
    "\n",
    "Enhances Stability → Reduces sensitivity to noisy or irrelevant features.\n",
    "\n",
    "Feature Selection → L1 regularization automatically drops unimportant features.\n",
    "\n",
    "Example:\n",
    "Without regularization, boosting might keep adding deep trees until it perfectly fits the training set (low bias, high variance).\n",
    "With lambda=1 and alpha=0.5, XGBoost prunes unnecessary complexity, achieving a better bias-variance trade-off.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ec6e6-c990-4beb-b9d9-d1352b758fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
    "\n",
    "Why CatBoost is Efficient for Handling Categorical Data\n",
    "CatBoost is a gradient boosting algorithm developed by Yandex that is specifically optimized for categorical features.\n",
    "It avoids the heavy manual preprocessing needed in most machine learning models.\n",
    "\n",
    "Key Reasons CatBoost Handles Categorical Data Well\n",
    "No Need for Manual One-Hot Encoding\n",
    "\n",
    "In most algorithms (e.g., XGBoost, LightGBM), you must convert categorical features into numerical form (often one-hot encoding), which:\n",
    "\n",
    "Increases dimensionality.\n",
    "\n",
    "Wastes memory and slows training.\n",
    "\n",
    "CatBoost automatically processes categorical features without expanding the feature space.\n",
    "\n",
    "Uses “Ordered Target Statistics” Encoding\n",
    "\n",
    "CatBoost converts a categorical value into a numeric statistic (e.g., mean target value) but does it in an “ordered” way:\n",
    "\n",
    "For each row, the encoding is computed using only previous rows (in a random permutation) — avoiding target leakage.\n",
    "\n",
    "This preserves the natural distribution of the data while preventing overfitting.\n",
    "\n",
    "Handles High-Cardinality Features Efficiently\n",
    "\n",
    "Features like city_name or product_id with thousands of unique values are processed efficiently using hashing + statistical encoding.\n",
    "\n",
    "Avoids memory blow-up from one-hot encoding.\n",
    "\n",
    "Symmetric Tree Structure\n",
    "\n",
    "CatBoost builds symmetric (oblivious) decision trees, meaning all leaves at the same depth use the same splitting rule.\n",
    "\n",
    "This allows fast training and inference, even with many categorical features.\n",
    "\n",
    "Built-in Support for Missing Values in Categorical Features\n",
    "\n",
    "Automatically handles NaN or unknown categories without manual imputation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5252f-db38-48e1-9932-73fe95a19e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature: Payment_Method = [Credit Card, Debit Card, PayPal, UPI]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f59fb5-00a1-477a-803f-0f945ee93c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost will:\n",
    "\n",
    "Internally transform these values into ordered numerical statistics based on the target variable.\n",
    "\n",
    "Use them directly in tree building without creating 4 dummy variables.\n",
    "\n",
    " Summary:\n",
    "CatBoost is efficient for categorical data because it:\n",
    "\n",
    "Avoids one-hot encoding.\n",
    "\n",
    "Uses leakage-free target statistics.\n",
    "\n",
    "Handles high-cardinality categories well.\n",
    "\n",
    "Builds fast, symmetric trees.\n",
    "\n",
    "Deals with missing categories automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816df15c-78d5-4224-8203-a09b1144b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5: What are some real-world applications where boosting techniques are\n",
    "preferred over bagging methods?\n",
    "\n",
    "\n",
    "Real-World Applications Where Boosting is Preferred Over Bagging\n",
    "Boosting techniques (like XGBoost, LightGBM, CatBoost, AdaBoost) are often preferred over bagging methods (Random Forest, BaggingClassifier) when:\n",
    "\n",
    "The dataset is complex and has non-linear relationships.\n",
    "\n",
    "The goal is high predictive accuracy rather than just reducing variance.\n",
    "\n",
    "The model needs to focus on hard-to-classify cases.\n",
    "\n",
    "1. Credit Risk & Loan Default Prediction\n",
    "Why Boosting? Boosting iteratively focuses on borrowers likely to default, capturing subtle risk patterns in financial + transactional data.\n",
    "\n",
    "Benefit: Reduces costly false negatives (missed defaults).\n",
    "\n",
    "2. Fraud Detection (Banking, E-commerce)\n",
    "Why Boosting? Fraud cases are rare and patterns are complex. Boosting’s focus on misclassified cases improves recall for rare events.\n",
    "\n",
    "Benefit: Detects fraudulent transactions with higher accuracy.\n",
    "\n",
    "3. Customer Churn Prediction (Telecom, SaaS)\n",
    "Why Boosting? Churn signals are hidden in small behavioral changes; boosting captures these by repeatedly focusing on borderline customers.\n",
    "\n",
    "Benefit: Enables targeted retention strategies.\n",
    "\n",
    "4. Medical Diagnosis & Disease Prediction\n",
    "Why Boosting? Medical datasets often have imbalance (rare diseases) and noisy features. Boosting handles these better than bagging.\n",
    "\n",
    "Benefit: Improves sensitivity for rare disease detection.\n",
    "\n",
    "5. Search Engine Ranking & Recommendation Systems\n",
    "Why Boosting? Gradient boosting is used in Learning to Rank (e.g., XGBoost in Bing, CatBoost in Yandex) because it can optimize ranking metrics directly.\n",
    "\n",
    "Benefit: More relevant search results & recommendations.\n",
    "\n",
    "6. Click-Through Rate (CTR) Prediction in Ads\n",
    "Why Boosting? Small variations in user behavior matter; boosting excels at modeling such subtle interactions.\n",
    "\n",
    "Benefit: Higher ad targeting accuracy, more revenue.\n",
    "\n",
    "7. Image Classification with Tabular Metadata\n",
    "Why Boosting? When image features are combined with additional categorical/tabular data, boosting can model the tabular part effectively.\n",
    "\n",
    "Benefit: Improved accuracy compared to bagging alone.\n",
    "\n",
    " Summary:\n",
    "Boosting is preferred over bagging when accuracy is the top priority, the patterns are complex, and rare or borderline cases matter.\n",
    "Bagging is better when variance reduction and stability are the main goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe51c3-ed7e-4cfc-bc54-c99bf4f53bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets:\n",
    "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
    "● Use sklearn.datasets.fetch_california_housing() for regression\n",
    "tasks.\n",
    "Question 6: Write a Python program to:\n",
    "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
    "● Print the model accuracy\n",
    "(Include your Python code and output in the code box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bdb6f1-d3e2-41c6-b897-75ab90f13ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create AdaBoost Classifier\n",
    "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 4. Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 7. Print results\n",
    "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45279d57-d73b-48c4-871d-50350aa1c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost Classifier Accuracy: 0.9649122807017544\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dc781-4b0c-47de-a76d-938bcd60165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 7: Write a Python program to:\n",
    "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
    "● Evaluate performance using R-squared score\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002125c2-b695-41ef-8235-7395550e7541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Load California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate performance using R-squared score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Gradient Boosting Regressor R-squared score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e2bd5-a167-43a9-ab1c-6c7cdd3600c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting Regressor R-squared score: 0.813507945896579\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a588a21-8eb7-4490-b373-9b8a2be8039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8: Write a Python program to:\n",
    "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
    "● Tune the learning rate using GridSearchCV\n",
    "● Print the best parameters and accuracy\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcbea9f-2d44-468e-a2e5-ec5941261b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 1. Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create XGBoost Classifier\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# 4. Parameter grid for learning rate tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# 5. GridSearchCV for tuning\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 6. Fit model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 7. Best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# 8. Best model accuracy\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff76312-7299-444b-9ab0-3516bcd16955",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Parameters: {'learning_rate': 0.1}\n",
    "Accuracy: 0.9736842105263158\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a515c-e321-4285-8981-42c7a83db0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9: Write a Python program to:\n",
    "● Train a CatBoost Classifier\n",
    "● Plot the confusion matrix using seaborn\n",
    "(Include your Python code and output in the code box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b9b3a-6ada-43cd-92f9-e777d3d6f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from catboost import CatBoostClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create and train CatBoost Classifier\n",
    "model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 5. Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 6. Plot using Seaborn\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=data.target_names,\n",
    "            yticklabels=data.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('CatBoost Classifier - Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e9330-50cc-42c8-8f8b-ccdd1a4c1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 10: You're working for a FinTech company trying to predict loan default using\n",
    "customer demographics and transaction behavior.\n",
    "The dataset is imbalanced, contains missing values, and has both numeric and\n",
    "categorical features.\n",
    "Describe your step-by-step data science pipeline using boosting techniques:\n",
    "● Data preprocessing & handling missing/categorical values\n",
    "● Choice between AdaBoost, XGBoost, or CatBoost\n",
    "● Hyperparameter tuning strategy\n",
    "● Evaluation metrics you'd choose and why\n",
    "● How the business would benefit from your model\n",
    "(Include your Python code and output in the code box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ced91-26eb-4fd3-a8b6-92772e16fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step-by-Step Data Science Pipeline\n",
    "1. Data Preprocessing\n",
    "Handling Missing Values:\n",
    "\n",
    "Numeric: Fill with median or use boosting’s built-in handling (XGBoost, CatBoost).\n",
    "\n",
    "Categorical: Fill with \"Unknown\" or let CatBoost handle them natively.\n",
    "\n",
    "Encoding Categorical Variables:\n",
    "\n",
    "XGBoost/AdaBoost: Apply One-Hot or Target Encoding.\n",
    "\n",
    "CatBoost: No manual encoding needed.\n",
    "\n",
    "Scaling: Not necessary for tree-based boosting methods.\n",
    "\n",
    "Imbalanced Data:\n",
    "\n",
    "Use scale_pos_weight (XGBoost), class weights, or oversampling (SMOTE).\n",
    "\n",
    "2. Choice of Algorithm\n",
    "CatBoost → Best for mixed numeric + categorical features, handles missing values automatically, avoids heavy preprocessing.\n",
    "\n",
    "XGBoost → Strong general-purpose boosting, needs preprocessing for categorical data.\n",
    "\n",
    "AdaBoost → Simpler, but less effective for highly imbalanced, complex datasets.\n",
    "\n",
    "Decision:\n",
    "Use CatBoost for this FinTech problem because:\n",
    "\n",
    "It directly handles categorical variables and missing values.\n",
    "\n",
    "It’s efficient and less prone to overfitting on small to medium-sized datasets.\n",
    "\n",
    "3. Hyperparameter Tuning Strategy\n",
    "Use GridSearchCV or RandomizedSearchCV over:\n",
    "\n",
    "iterations (number of trees)\n",
    "\n",
    "depth (tree depth)\n",
    "\n",
    "learning_rate\n",
    "\n",
    "l2_leaf_reg (L2 regularization)\n",
    "\n",
    "border_count (number of splits for numeric features)\n",
    "\n",
    "Include early stopping for faster convergence.\n",
    "\n",
    "4. Evaluation Metrics\n",
    "Primary:\n",
    "\n",
    "AUC-ROC → Measures ranking ability; robust to imbalance.\n",
    "\n",
    "Precision, Recall, F1-score → Important for loan default risk.\n",
    "\n",
    "Why:\n",
    "\n",
    "High Recall ensures you catch most defaulters.\n",
    "\n",
    "High Precision ensures you don’t wrongly flag too many safe customers.\n",
    "\n",
    "5. Business Benefits\n",
    "Reduces loan defaults by identifying risky applicants early.\n",
    "\n",
    "Improves credit risk assessment without increasing rejection rates unnecessarily.\n",
    "\n",
    "Enables personalized loan offers based on risk profile.\n",
    "\n",
    "Enhances regulatory compliance by explaining decisions (feature importance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c2889-b708-4eb2-ac3a-0a1bcd7c7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example synthetic dataset (replace with real loan data)\n",
    "np.random.seed(42)\n",
    "data_size = 1000\n",
    "df = pd.DataFrame({\n",
    "    'age': np.random.randint(21, 70, data_size),\n",
    "    'income': np.random.randint(20000, 150000, data_size),\n",
    "    'loan_amount': np.random.randint(5000, 50000, data_size),\n",
    "    'employment_type': np.random.choice(['Salaried', 'Self-Employed', 'Unemployed'], data_size),\n",
    "    'has_credit_card': np.random.choice(['Yes', 'No'], data_size),\n",
    "    'transaction_volume': np.random.randint(1, 100, data_size),\n",
    "    'default': np.random.choice([0, 1], data_size, p=[0.85, 0.15])  # imbalanced\n",
    "})\n",
    "\n",
    "# Introduce some missing values\n",
    "df.loc[np.random.choice(df.index, 50), 'income'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 30), 'employment_type'] = np.nan\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('default', axis=1)\n",
    "y = df['default']\n",
    "\n",
    "# Identify categorical features\n",
    "cat_features = ['employment_type', 'has_credit_card']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Create CatBoost Pool\n",
    "train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "test_pool = Pool(X_test, y_test, cat_features=cat_features)\n",
    "\n",
    "# Model with basic params\n",
    "model = CatBoostClassifier(verbose=0, random_state=42, eval_metric='AUC')\n",
    "\n",
    "# Hyperparameter tuning with RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'iterations': [200, 500, 800],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    n_iter=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train, cat_features=cat_features)\n",
    "\n",
    "# Best params\n",
    "print(\"Best Parameters:\", search.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "best_model = search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
